{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfdd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4684d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "notes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "accs = [\"b\", \"s\", \"\"]\n",
    "all_notes_list = [note + acc for note in notes for acc in accs]\n",
    "\n",
    "keys_list = [\"C\", \"Cs\", \"D\", \"Ds\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"As\", \"B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a88c",
   "metadata": {},
   "source": [
    "Simplifying chords down to 42: base note (A-G) + accidental + major/minor(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a3bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning ###\n",
    "def simplify_chord(chord: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes chord quality from a chord.\n",
    "    \"\"\"\n",
    "    for note in all_notes_list:\n",
    "        if not chord.startswith(note):\n",
    "            continue\n",
    "\n",
    "        suffix = chord.removeprefix(note)\n",
    "        if suffix.startswith(\"min\") or suffix.startswith(\"dim\"):\n",
    "            return note + \"min\"\n",
    "        else:\n",
    "            return note\n",
    "\n",
    "    if chord == \"sC\":\n",
    "        return \"Cs\"\n",
    "\n",
    "    # print(chord)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189d77b",
   "metadata": {},
   "source": [
    "Transpose each song to mitigate bias from the key of the song and to augment our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70328e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transposing songs to all keys ###\n",
    "def transpose_chord(chord: str, variation: int) -> str:\n",
    "    # Find the base note and suffix\n",
    "    for note in keys_list:\n",
    "        if chord.startswith(note):\n",
    "            suffix = chord.removeprefix(note)\n",
    "            idx = keys_list.index(note)\n",
    "            new_note = keys_list[(idx + variation) % 12]\n",
    "            return new_note + suffix\n",
    "    return chord  # If not found, return as is\n",
    "\n",
    "def augment_keys(df):\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        for variation in range(12):\n",
    "            new_row = row.copy()\n",
    "            if variation == 0:\n",
    "                new_row[\"original_key\"] = True\n",
    "            else:\n",
    "                new_row[\"original_key\"] = False\n",
    "            new_row[\"added_semitones\"] = variation\n",
    "            new_row[\"chords\"] = [transpose_chord(chord, variation) for chord in row[\"chords\"]]\n",
    "            augmented_rows.append(new_row)\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "def standardize_chord_prefix(chord: str) -> str:\n",
    "    # Map flat notes to their sharp equivalents\n",
    "    flat_to_sharp = {\n",
    "        \"Bb\": \"As\",\n",
    "        \"Db\": \"Cs\",\n",
    "        \"Eb\": \"Ds\",\n",
    "        \"Gb\": \"Fs\",\n",
    "        \"Ab\": \"Gs\"\n",
    "    }\n",
    "    for flat, sharp in flat_to_sharp.items():\n",
    "        if chord.startswith(flat):\n",
    "            return sharp + chord[len(flat):]\n",
    "    return chord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79ae1f",
   "metadata": {},
   "source": [
    "Calculate n-gram counts. Second method uses CountVectorizer library (although its use case is bag of n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f11b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating n-gram counts ###\n",
    "def load_chord_data(series) -> tuple[dict[str, int], dict[str, dict[str, int]]]:\n",
    "    unigram_dict: dict[str, int] = {}\n",
    "    bigram_dict: dict[str, dict[str, int]] = {}\n",
    "\n",
    "    for row in series:\n",
    "        # Unigram counts\n",
    "        for chord in row:\n",
    "            unigram_dict[chord] = unigram_dict.get(chord, 0) + 1\n",
    "        # Bigram counts\n",
    "        for i in range(len(row) - 1):\n",
    "            w1, w2 = row[i], row[i + 1]\n",
    "            if w1 not in bigram_dict:\n",
    "                bigram_dict[w1] = {}\n",
    "            bigram_dict[w1][w2] = bigram_dict[w1].get(w2, 0) + 1\n",
    "\n",
    "    return unigram_dict, bigram_dict\n",
    "\n",
    "def count_n_grams(data, n: int = 1) -> pd.DataFrame:\n",
    "    word_vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, n),\n",
    "        analyzer=\"word\",\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "        lowercase=False,\n",
    "    )\n",
    "\n",
    "    sparse_matrix = word_vectorizer.fit_transform(\n",
    "        data.map(lambda chords: \" \".join(chords))\n",
    "    )\n",
    "\n",
    "    frequencies = sum(sparse_matrix).toarray()[0]\n",
    "\n",
    "    df_all = pd.DataFrame(\n",
    "        frequencies,\n",
    "        index=word_vectorizer.get_feature_names_out(),\n",
    "        columns=[\"count\"],\n",
    "    )\n",
    "\n",
    "    return df_all.groupby(by=lambda chords: len(chords.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950316",
   "metadata": {},
   "source": [
    "Likelihood calculation given a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a32d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Song log likelihood calculation ###\n",
    "def song_log_likelihood_ngram(song, n, ngram_probs):\n",
    "    # song: list of chords in song\n",
    "    # n: order of the n-gram model\n",
    "    # ngram_probs: dict[context_tuple] -> dict[target] = P(target | context)\n",
    "    # ex: trigram ngram_prob = dict[(chord1, chord2)] = {chord0:P,...,chordV:P}, dict[chord3] = P(chord3 | chord1, chord2)\n",
    "    # vocab_size: 42 or 36?\n",
    "\n",
    "    ll = 0.0\n",
    "    if len(song) < n:\n",
    "        return 0.0\n",
    "\n",
    "    for t in range(n-1, len(song)):\n",
    "        context = tuple(song[t-(n-1):t])\n",
    "        target = song[t]\n",
    "\n",
    "        if context in ngram_probs:\n",
    "            p = ngram_probs[context].get(target, 0.0)\n",
    "        else:\n",
    "            p = 1e-12\n",
    "\n",
    "        if p <= 0:\n",
    "            p = 1e-12\n",
    "        ll += np.log(p)\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c8/zh68_k6j30l283wd6sjg39280000gn/T/ipykernel_11358/2972707129.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"hf://datasets/ailsntua/Chordonomicon/chordonomicon_v2.csv\",usecols=[\"chords\", \"main_genre\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [C, F, C, E, Amin, C, F, C, G, C, F, C, E, Ami...\n",
      "4    [C, G, C, G, C, F, Dmin, G, Dmin, G, C, G, C, ...\n",
      "6    [G, Bmin, Amin, D, G, Bmin, Amin, D, G, Emin, ...\n",
      "7    [Fsmin, Fs, B, E, Fs, B, E, Fsmin, B, As, Gsmi...\n",
      "8    [C, Amin, Dmin, G, C, G, Amin, Dmin, G, C, Dmi...\n",
      "Name: chords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# original Series (before transposing code)\n",
    "# df = pd.read_csv(\"hf://datasets/ailsntua/Chordonomicon/chordonomicon_v2.csv\",usecols=[\"chords\", \"main_genre\"])\n",
    "\n",
    "# pop_chords = df[df[\"main_genre\"] == \"pop\"][\"chords\"]\n",
    "# pop_chords = pop_chords.str.split(\" \")\n",
    "# pop_chords = pop_chords.map(\n",
    "#     lambda chords: [simplify_chord(chord) for chord in chords if not chord.startswith(\"<\")]\n",
    "# )\n",
    "\n",
    "# print(pop_chords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing transposed dataset to df\n",
    "# df = pd.read_csv(\"chordonomicon_v2.csv\", usecols=[\"id\", \"chords\", \"main_genre\"])\n",
    "# pop_df = df[df[\"main_genre\"] == \"pop\"][[\"id\", \"chords\"]].copy()\n",
    "# pop_df[\"chords\"] = pop_df[\"chords\"].str.split(\" \")\n",
    "# pop_df[\"chords\"] = pop_df[\"chords\"].map(\n",
    "#     lambda chords: [chord for chord in chords if not chord.startswith(\"<\")]\n",
    "# )\n",
    "# pop_df[\"chords\"] = pop_df[\"chords\"].map(\n",
    "#     lambda chords: [simplify_chord(standardize_chord_prefix(chord)) for chord in chords]\n",
    "# )\n",
    "# pop_df[\"original_key\"] = True\n",
    "# pop_df[\"added_semitones\"] = 0\n",
    "\n",
    "# augmented_df = augment_keys(pop_df)\n",
    "# print(augmented_df.head(15))\n",
    "\n",
    "# augmented_df.to_csv(\"chordonomicon_v2_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb32dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count\n",
      "Gsbmin    57360\n",
      "Gbmin     57360\n",
      "Cbmin     57360\n",
      "Dbmin     57360\n",
      "Dsbmin    57360\n",
      "...         ...\n",
      "Cs      4408798\n",
      "Gs      4408798\n",
      "As      4408798\n",
      "Fs      4408798\n",
      "Ds      4408798\n",
      "\n",
      "[62 rows x 1 columns] \n",
      "\n",
      "\n",
      "['A', 'Ab', 'Abmin', 'Amin', 'As', 'Asb', 'Asbmin', 'Asmin', 'Ass', 'Assmin', 'B', 'Bb', 'Bbmin', 'Bmin', 'Bs', 'Bsmin', 'C', 'Cb', 'Cbmin', 'Cmin', 'Cs', 'Csb', 'Csbmin', 'Csmin', 'Css', 'Cssmin', 'D', 'Db', 'Dbmin', 'Dmin', 'Ds', 'Dsb', 'Dsbmin', 'Dsmin', 'Dss', 'Dssmin', 'E', 'Eb', 'Ebmin', 'Emin', 'Es', 'Esmin', 'F', 'Fb', 'Fbmin', 'Fmin', 'Fs', 'Fsb', 'Fsbmin', 'Fsmin', 'Fss', 'Fssmin', 'G', 'Gb', 'Gbmin', 'Gmin', 'Gs', 'Gsb', 'Gsbmin', 'Gsmin', 'Gss', 'Gssmin']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "pop_chords = pd.read_csv('chordonomicon_v2_augmented.csv', usecols=['chords'])['chords'].apply(ast.literal_eval) # transposed pop songs\n",
    "\n",
    "n = 1\n",
    "\n",
    "# Get counts for all n-grams\n",
    "n_gram_counts = count_n_grams(pop_chords, n)\n",
    "# print(n_gram_counts)\n",
    "for key, item in n_gram_counts:\n",
    "    print(n_gram_counts.get_group(key).sort_values(by='count'), \"\\n\\n\")\n",
    "\n",
    "observed_chords = sorted(pop_chords.map(set).agg(lambda x: set.union(*x))) # vocubulary\n",
    "print(observed_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0 # Laplace smoothing\n",
    "\n",
    "# Calculate transition matrix probabilities\n",
    "unigram = n_gram_counts.get_group(1)\n",
    "unigram = unigram.reindex(all_notes_list, fill_value=0)\n",
    "unigram[\"prob\"] = (unigram[\"count\"] + alpha) / (unigram[\"count\"].sum() + alpha * len(observed_chords))\n",
    "\n",
    "bigram = n_gram_counts.get_group(2)\n",
    "bigram[\"evidence\"] = bigram.index.map(lambda s: s.split()[0]) # get (n-1)-length evidence\n",
    "bigram[\"next\"] = bigram.index.map(lambda s: s.split()[1]) # next chords\n",
    "\n",
    "full_index = pd.MultiIndex.from_product([all_notes_list, all_notes_list], names=[\"evidence\", \"next\"])\n",
    "bigram = bigram.set_index([\"evidence\", \"next\"])\n",
    "bigram = bigram.reindex(full_index, fill_value=0)\n",
    "\n",
    "evidence_counts = bigram[\"count\"].groupby(level=\"evidence\").transform(\"sum\")\n",
    "bigram[\"prob\"] = (bigram[\"count\"] + alpha) / (evidence_counts + alpha * len(bigram[\"next\"].unique()))\n",
    "\n",
    "# 2d dataframe\n",
    "transition_matrix = bigram[\"prob\"].unstack(fill_value=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
